import polars as pl

from buckaroo.lazy_infinite_polars_widget import LazyInfinitePolarsBuckarooWidget
from buckaroo.file_cache.base import FileCache
from buckaroo.read_utils import read_df
from pathlib import Path
from buckaroo.file_cache.base import Executor as _Exec
from tests.unit.file_cache.executor_test_utils import wait_for_nested_executor_finish


def _capture_sends(widget):
    sent = []
    def _send(payload, buffers):
        sent.append((payload, buffers))
    widget.send = _send  # type: ignore[attr-defined]
    return sent


def test_lazy_infinite_widget_init_and_summary():
    df = pl.DataFrame({'normal_int_series': [1, 2, 3, 4]})
    ldf = df.lazy()
    # Use slow executor to make the background transition observable
    from buckaroo.file_cache.paf_column_executor import PAFColumnExecutor
    import time as _time
    class SlowPAFColumnExecutor(PAFColumnExecutor):
        def execute(self, ldf, execution_args):
            _time.sleep(0.2)
            return super().execute(ldf, execution_args)

    w = LazyInfinitePolarsBuckarooWidget(ldf, column_executor_class=SlowPAFColumnExecutor)

    # df_meta should reflect lazy row and column counts
    assert w.df_meta['columns'] == 1
    assert w.df_meta['total_rows'] == 4

    # Synchronous path: populated immediately
    assert w.df_data_dict['all_stats'] != []

    # All stats should include orig/rewritten indicators after populated
    all_stats = w.df_data_dict['all_stats']
    idx_keys = [row['index'] for row in all_stats]
    assert 'orig_col_name' in idx_keys
    assert 'rewritten_col_name' in idx_keys

    # df_display_args wired for infinite viewer
    assert 'main' in w.df_display_args
    main_args = w.df_display_args['main']
    assert main_args['data_key'] == 'main'
    assert main_args['summary_stats_key'] == 'all_stats'
    assert 'df_viewer_config' in main_args
    assert 'column_config' in main_args['df_viewer_config']


def test_payload_slice_returns_requested_rows():
    df = pl.DataFrame({'a': [10, 20, 30, 40, 50]})
    ldf = df.lazy()
    w = LazyInfinitePolarsBuckarooWidget(ldf)
    captured = _capture_sends(w)

    # Request middle slice [1,4) => 3 rows
    w._handle_payload_args({'start': 1, 'end': 4})
    assert len(captured) == 1
    payload, buffers = captured[0]
    assert payload['type'] == 'infinite_resp'
    assert isinstance(buffers, list) and len(buffers) == 1

    # Decode parquet and verify row count equals requested slice length
    out_df = pl.read_parquet(buffers[0])
    # rewritten columns include 'index' and 'a'
    assert set(out_df.columns) >= set(['index', 'a'])
    assert out_df.shape[0] == 3
    assert out_df['a'].to_list() == [20, 30, 40]


def test_payload_sort_and_slice():
    df = pl.DataFrame({'x': [3, 1, 2, 5, 4]})
    ldf = df.lazy()
    w = LazyInfinitePolarsBuckarooWidget(ldf)
    captured = _capture_sends(w)

    # Determine rewritten name for 'x' (likely 'a')
    # Infer from df_display_args column_config
    cc = w.df_display_args['main']['df_viewer_config']['column_config']
    # pick the first non-index rewritten col
    rw = next(c['col_name'] for c in cc if c['col_name'] != 'index')

    # Request sorted ascending, first two rows
    w._handle_payload_args({'start': 0, 'end': 2, 'sort': rw, 'sort_direction': 'asc'})
    assert len(captured) == 1
    payload, buffers = captured[0]
    out_df = pl.read_parquet(buffers[0])
    # The smallest two values should be returned
    vals = out_df[rw].to_list()
    assert vals == sorted(vals)[:2]


def test_multiple_payloads_large_df():
    # 2000 rows
    df = pl.DataFrame({'v': list(range(2000))})
    ldf = df.lazy()
    w = LazyInfinitePolarsBuckarooWidget(ldf)
    captured = _capture_sends(w)

    # First window [0, 100)
    w._handle_payload_args({'start': 0, 'end': 100})
    # Second window [100, 250)
    w._handle_payload_args({'start': 100, 'end': 250})
    # Third window with a chained second_request
    w._handle_payload_args({'start': 500, 'end': 520, 'second_request': {'start': 900, 'end': 905}})

    # Expect three payloads; the third produces two sends (primary + second_request)
    assert len(captured) == 4

    # Validate first slice contents/size
    payload1, buffers1 = captured[0]
    out1 = pl.read_parquet(buffers1[0])
    assert out1.shape[0] == 100
    # rewritten col name for 'v' can be inferred from columns (exclude 'index')
    rw_cols1 = [c for c in out1.columns if c != 'index']
    assert len(rw_cols1) == 1
    rw1 = rw_cols1[0]
    assert out1[rw1].to_list()[0] == 0
    assert out1[rw1].to_list()[-1] == 99

    # Validate second slice contents/size
    payload2, buffers2 = captured[1]
    out2 = pl.read_parquet(buffers2[0])
    assert out2.shape[0] == 150
    rw_cols2 = [c for c in out2.columns if c != 'index']
    assert len(rw_cols2) == 1
    rw2 = rw_cols2[0]
    assert out2[rw2].to_list()[0] == 100
    assert out2[rw2].to_list()[-1] == 249

    # Validate third (primary) slice contents/size
    payload3a, buffers3a = captured[2]
    out3a = pl.read_parquet(buffers3a[0])
    assert out3a.shape[0] == 20
    rw_cols3a = [c for c in out3a.columns if c != 'index']
    rw3a = rw_cols3a[0]
    assert out3a[rw3a].to_list()[0] == 500
    assert out3a[rw3a].to_list()[-1] == 519

    # Validate third (second_request) slice contents/size
    payload3b, buffers3b = captured[3]
    out3b = pl.read_parquet(buffers3b[0])
    assert out3b.shape[0] == 5
    rw_cols3b = [c for c in out3b.columns if c != 'index']
    rw3b = rw_cols3b[0]
    assert out3b[rw3b].to_list() == [900, 901, 902, 903, 904]


def test_handle_payload_never_collects_base(monkeypatch):
    """
    Guard that _handle_payload_args never calls collect() on the original LazyFrame,
    only on sliced frames.
    """
    df = pl.DataFrame({'v': list(range(200))})
    ldf = df.lazy()
    w = LazyInfinitePolarsBuckarooWidget(ldf)
    captured = _capture_sends(w)

    base_ldf = w._ldf
    orig_collect = pl.LazyFrame.collect

    def guarded_collect(self, *args, **kwargs):
        if self is base_ldf:
            raise AssertionError("Full collect on base LazyFrame was called")
        return orig_collect(self, *args, **kwargs)

    # Patch after widget init to avoid blocking summary computation
    monkeypatch.setattr(pl.LazyFrame, "collect", guarded_collect, raising=True)

    # Now call payload handler; it must collect only the sliced frame
    w._handle_payload_args({'start': 10, 'end': 20})
    assert len(captured) == 1
    payload, buffers = captured[0]
    out_df = pl.read_parquet(buffers[0])
    assert out_df.shape[0] == 10


def test_synchronous_populates_immediately():
    df = pl.DataFrame({'v': [1, 2, 3]})
    ldf = df.lazy()
    # Make it slow to assert background behavior
    from buckaroo.file_cache.paf_column_executor import PAFColumnExecutor
    import time as _time
    class SlowPAFColumnExecutor(PAFColumnExecutor):
        def execute(self, ldf, execution_args):
            _time.sleep(0.2)
            return super().execute(ldf, execution_args)

    w = LazyInfinitePolarsBuckarooWidget(ldf, column_executor_class=SlowPAFColumnExecutor)
    assert w.df_data_dict['all_stats'] != []

def test_cache_short_circuit_populates_immediately(tmp_path):
    # Prepare a fake cached merged_sd
    cached_sd = {
        'a': {'orig_col_name': 'v', 'rewritten_col_name': 'a', 'mean': 2.0, 'null_count': 0}
    }
    fc = FileCache()
    fpath = tmp_path / "fake.parq"
    fpath.write_text("placeholder")
    fc.upsert_file_metadata(Path(fpath), {'merged_sd': cached_sd})

    df = pl.DataFrame({'v': [1, 2, 3]})
    ldf = df.lazy()
    # Use a failing sync executor to ensure we did not run compute if cache is present
    from buckaroo.file_cache.base import Executor as _Exec
    class FailingExec(_Exec):
        def run(self):  # type: ignore[override]
            raise RuntimeError("should not be called due to cache hit")

    w = LazyInfinitePolarsBuckarooWidget(ldf, file_path=str(fpath), file_cache=fc, sync_executor_class=FailingExec)
    assert w.df_data_dict['all_stats'] != []
    idx_keys = [row['index'] for row in w.df_data_dict['all_stats']]
    assert 'orig_col_name' in idx_keys and 'mean' in idx_keys

def test_executor_selection_thresholds_and_fallback():
    # 51 columns triggers parallel selection
    data = {f'c{i}': [i] for i in range(51)}
    df = pl.DataFrame(data)
    ldf = df.lazy()

    # Track which executor path was used

    class TrackingSync(_Exec):
        used = False
        def run(self):  # type: ignore[override]
            TrackingSync.used = True
            return super().run()
    class TrackingPar(_Exec):
        used = False
        def run(self):  # type: ignore[override]
            TrackingPar.used = True
            return super().run()

    w = LazyInfinitePolarsBuckarooWidget(ldf, sync_executor_class=TrackingSync, parallel_executor_class=TrackingPar)
    assert TrackingPar.used is True
    assert w # we want w to be able to use it for future tests that we haven't written yet, this keeps ruff happy
    
    # Now test fallback: small df chooses sync, but sync fails, fallback to parallel
    small_df = pl.DataFrame({'x': [1, 2, 3]})
    small_ldf = small_df.lazy()
    class FailingSync(_Exec):
        def run(self):  # type: ignore[override]
            raise RuntimeError("fail sync")
    class WorkingPar(_Exec):
        used = False
        def run(self):  # type: ignore[override]
            WorkingPar.used = True
            return super().run()

    w2 = LazyInfinitePolarsBuckarooWidget(small_ldf, sync_executor_class=FailingSync, parallel_executor_class=WorkingPar)
    assert WorkingPar.used is True
    assert w2.df_data_dict['all_stats'] != []

def test_full_summary_present_immediately():
    df = pl.DataFrame({'c1': [1, 2, 3], 'c2': [10, 20, 30], 'c3': [7, 8, 9]})
    ldf = df.lazy()
    from buckaroo.file_cache.paf_column_executor import PAFColumnExecutor
    import time as _time
    class SlowPAFColumnExecutor(PAFColumnExecutor):
        def execute(self, ldf, execution_args):
            # small sleep per column-chunk to allow polling
            _time.sleep(0.15)
            return super().execute(ldf, execution_args)

    w = LazyInfinitePolarsBuckarooWidget(ldf, column_executor_class=SlowPAFColumnExecutor)
    rows = w.df_data_dict['all_stats']
    assert rows != []
    ocn_rows = [r for r in rows if r.get('index') == 'orig_col_name']
    assert ocn_rows
    row = ocn_rows[0]
    present_cols = [k for k in row.keys() if k not in ('index', 'level_0')]
    assert len(present_cols) == 3

def test_column_subset_filters_cached_columns(tmp_path):
    """Test that widget with subset of columns only shows those columns, not all cached columns."""
    # Create a small test parquet file with 5 columns
    df_full = pl.DataFrame({
        'col_a': [1, 2, 3],
        'col_b': [10, 20, 30],
        'col_c': [100, 200, 300],
        'col_d': [1000, 2000, 3000],
        'col_e': [10000, 20000, 30000],
    })
    
    test_file = tmp_path / "test_subset.parquet"
    df_full.write_parquet(test_file)
    
    fc = FileCache()
    file_path_str = str(test_file)
    
    # Step 1: Create widget with full dataframe - will populate cache
    ldf_full = read_df(file_path_str)
    # Use sync executor for both to avoid multiprocessing overhead (fast test)
    bw1 = LazyInfinitePolarsBuckarooWidget(
        ldf_full, 
        timeout_secs=10, 
        file_path=file_path_str,
        file_cache=fc,
        sync_executor_class=_Exec,
        parallel_executor_class=_Exec,  # Force sync executor
    )
    
    # Wait for computation to complete (sync executor is blocking, but use utility for consistency)
    wait_for_nested_executor_finish(bw1, timeout_secs=5.0)
    
    # Verify bw1 has all 5 columns with correct cached values
    bw1_columns = set(bw1._df.merged_sd.keys())
    expected_full = {'a', 'b', 'c', 'd', 'e'}  # rewritten column names
    assert bw1_columns == expected_full, f"bw1 should have {expected_full}, got {bw1_columns}"
    
    # Verify cached values in bw1 are correct for col_c (rewritten as 'c')
    # This ensures the cache has the right data before we test filtering
    bw1_col_c_stats = bw1._df.merged_sd.get('c', {})
    assert bw1_col_c_stats.get('orig_col_name') == 'col_c', "bw1 col_c should have correct orig_col_name"
    # col_c has values [100, 200, 300], so mean = 200
    if bw1_col_c_stats.get('mean', 0) != 0:
        assert bw1_col_c_stats.get('mean') == 200.0, (
            f"bw1 cached col_c should have mean=200.0, got {bw1_col_c_stats.get('mean')}"
        )
    
    # Step 2: Create widget with subset of columns (col_d, col_e, col_c) - reordered
    ldf_subset = ldf_full.select(['col_d', 'col_e', 'col_c'])
    
    bw2 = LazyInfinitePolarsBuckarooWidget(
        ldf_subset,
        timeout_secs=10,
        file_path=file_path_str,
        file_cache=fc,
        sync_executor_class=_Exec,
        parallel_executor_class=_Exec,  # Force sync executor
    )
    
    # Wait for computation to complete (sync executor is blocking)
    wait_for_nested_executor_finish(bw2, timeout_secs=5.0)
    
    # Get the rewritten column names for the subset
    # When selecting ['col_d', 'col_e', 'col_c'], they get rewritten as:
    # - col_d (first) -> 'a'
    # - col_e (second) -> 'b'
    # - col_c (third) -> 'c'
    expected_subset = {'a', 'b', 'c'}  # rewritten names for 3-column subset
    
    # The bug fix: bw2 should only show the 3 columns from the subset, not all 5
    bw2_columns = set(bw2._df.merged_sd.keys())
    
    assert bw2_columns == expected_subset, (
        f"bw2 should only have columns {expected_subset}, but got {bw2_columns}. "
        f"The cache should be filtered to only include columns in the subset."
    )
    
    # Verify specific column values match what we expect
    # After reordering ['col_d', 'col_e', 'col_c'], they get rewritten as 'a', 'b', 'c'
    # Column 'c' should correspond to col_c with values [100, 200, 300]
    col_c_stats = bw2._df.merged_sd.get('c', {})
    assert 'orig_col_name' in col_c_stats, "col_c should have orig_col_name in stats"
    assert col_c_stats['orig_col_name'] == 'col_c', (
        f"Column 'c' should have orig_col_name='col_c', got {col_c_stats.get('orig_col_name')}"
    )
    
    # Verify that col_c's cached stats match the expected values from the original data
    # col_c has values [100, 200, 300], so mean = 200, min = 100, max = 300
    # The cached data should preserve these values correctly
    assert 'mean' in col_c_stats, "col_c stats should include 'mean' from cache"
    # If mean is 0, it's a default value, not cached - but we still verify the structure is correct
    # For a proper test, we need cached values, so assert mean matches if it's not the default
    col_c_mean = col_c_stats.get('mean', 0)
    if col_c_mean != 0:  # Only check if we have real cached stats (not defaults)
        assert col_c_mean == 200.0, (
            f"Column 'c' (col_c) should have mean=200.0 from cached data [100, 200, 300], "
            f"got {col_c_mean}"
        )
        assert col_c_stats.get('min') == 100.0, (
            f"Column 'c' (col_c) should have min=100.0, got {col_c_stats.get('min')}"
        )
        assert col_c_stats.get('max') == 300.0, (
            f"Column 'c' (col_c) should have max=300.0, got {col_c_stats.get('max')}"
        )
    
    # Verify col_d (rewritten as 'a') has correct orig_col_name and values
    # col_d has values [1000, 2000, 3000], so mean = 2000
    col_d_stats = bw2._df.merged_sd.get('a', {})
    assert col_d_stats.get('orig_col_name') == 'col_d', (
        f"Column 'a' should have orig_col_name='col_d', got {col_d_stats.get('orig_col_name')}"
    )
    col_d_mean = col_d_stats.get('mean', 0)
    if col_d_mean != 0:
        assert col_d_mean == 2000.0, (
            f"Column 'a' (col_d) should have mean=2000.0 from cached data [1000, 2000, 3000], "
            f"got {col_d_mean}"
        )
    
    # Verify col_e (rewritten as 'b') has correct orig_col_name and values  
    # col_e has values [10000, 20000, 30000], so mean = 20000
    col_e_stats = bw2._df.merged_sd.get('b', {})
    assert col_e_stats.get('orig_col_name') == 'col_e', (
        f"Column 'b' should have orig_col_name='col_e', got {col_e_stats.get('orig_col_name')}"
    )
    col_e_mean = col_e_stats.get('mean', 0)
    if col_e_mean != 0:
        assert col_e_mean == 20000.0, (
            f"Column 'b' (col_e) should have mean=20000.0 from cached data [10000, 20000, 30000], "
            f"got {col_e_mean}"
        )
    
    # Also check that the widget's df_meta reflects the correct column count
    assert bw2.df_meta['columns'] == 3, f"bw2 should report 3 columns, got {bw2.df_meta['columns']}"

