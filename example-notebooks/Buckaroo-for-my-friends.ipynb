{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18dc926-43d4-4db0-9f59-07bef3fb40ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Explaining how Buckaroo fits into the PyData ecosystem for Friends and Family\n",
    "\n",
    "I want to epxlain for my non technical friends what I have been building, and how it fits into the larger Python Data Ecosystem.\n",
    "\n",
    "## How programming languages do Math\n",
    "\n",
    "Before we dive into exciting visuals and interactive programs, let’s lay some groundwork. In commonly used programming languages like Python, JavaScript, and Excel, mathematical operations can be slow. When evaluating an expression like `c = a + b`, the computer must check the types of a and b, then figure out how to add them. Other languages like `C` and `Fortran` allowed faster processing but are less user-friendly.\n",
    "\n",
    "## NumPy: Accelerating Matrix Math\n",
    "\n",
    "Quickly performing operations on matrices is essential for linear regression, image recognition, and AI (like ChatGPT). \n",
    "\n",
    "In 2006, Travis Oliphant created NumPy — a library specifically for arrays and matrices. In most programming languages, adding two lists of numbers involves type-checking every element.  NumPy determines the types of entire matrices, then efficiently adds their elements together. \n",
    "\n",
    "Let's see how much faster this is on a 1,000 x 1,000 matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26e9a7f0-4c06-4248-beee-75b8137acd1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.8 ms ± 650 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "ELEMENTS = 1_000_000\n",
    "py_a = [x for x in range(ELEMENTS)]\n",
    "py_b = [x for x in range(ELEMENTS*10, 0, -10)]\n",
    "%timeit -n 10 [py_a[i] + py_b[i] for i in range(ELEMENTS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b937620f-eb44-492f-ba3d-7506e2c89ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712 µs ± 295 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_a = np.arange(ELEMENTS)\n",
    "np_b = np.arange(ELEMENTS * 10, 0, -10)\n",
    "%timeit -n 100 np_a + np_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cfbd3-4edd-4ddb-a02c-03b565605d37",
   "metadata": {},
   "source": [
    "## How much faster\n",
    "37.8 milliseconds vs 895 microseconds, or about 50 times faster. A microsecond is a milionth of a second, a millisecond is a thousandth\n",
    "With NumPy, C/Fortran speed was accessible from the friendly language of python, in short easy to understand snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f193dc4b-e988-4daf-9d4a-51074e284b78",
   "metadata": {},
   "source": [
    "# The PyData ecosystem emerges\n",
    "\n",
    "NumPy was revolutionary because it made high performance numerical computing accesible to academics that needed to harness computation to power their analyses.  Computational Biologists, Physicists, electrical engineers, and Astophysiscists all used NumPy, and contributed back to this open source library.  They also wrote their own libraries \n",
    "\n",
    "* [SciPy](https://scipy.org/) for linear regressions, differential equations, and much more.  2001 - Travis Oliphant and many more\n",
    "* [Matplotlib](https://matplotlib.org/) for static plots 2003\n",
    "\n",
    "Here are these two tools together to build a linear regression chart. https://python-graph-gallery.com/scatterplot-with-regression-fit-in-matplotlib/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcb49e2-d44c-4a6c-9535-529db8704338",
   "metadata": {},
   "source": [
    "# Explaining Buckarooto my non-technical friends \n",
    "\n",
    "This notebook explains my side project Buckaroo, and how it fits into the datascience ecosystem.\n",
    "\n",
    "## Background on python and how it's used in datascience\n",
    "\n",
    "Before we dive into exciting visuals and interactive programs, let’s lay some groundwork.\n",
    "\n",
    "Python is a popular programming language, you might also have heard of java, C, and javascript.  Datascientists have come to rely on python because it balances speed of execution (C is faster) with ease of use and learning.  There are other open source libraries that Buckaroo leverages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26840cf2-8e50-4664-ad93-100827989534",
   "metadata": {},
   "source": [
    "\n",
    "### NumPy\n",
    "\n",
    "NumPy was written by Travis Oliphant in 2006.  Matrix math is at the heart of linear regression, image recognition, and AI like ChatGPT.  In many cases NumPy accelerates matrix operations to 25-100x faster than raw python.\n",
    "\n",
    "Here is NumPy and the Matplotlib charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805df0ba-4349-4ea3-a337-8a19cef0e2c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "rng = np.random.default_rng(1234)\n",
    "# Generate data\n",
    "x = rng.uniform(0, 10, size=100)\n",
    "y = x + rng.normal(size=100)\n",
    "\n",
    "# Initialize layout\n",
    "fig, ax = plt.subplots(figsize = (9, 9))\n",
    "\n",
    "# Add scatterplot\n",
    "ax.scatter(x, y, s=60, alpha=0.7, edgecolors=\"k\")\n",
    "\n",
    "# Fit linear regression via least squares with numpy.polyfit\n",
    "# It returns an slope (b) and intercept (a)\n",
    "# deg=1 means linear fit (i.e. polynomial of degree 1)\n",
    "b, a = np.polyfit(x, y, deg=1)\n",
    "\n",
    "# Create sequence of 100 numbers from 0 to 100 \n",
    "xseq = np.linspace(0, 10, num=100)\n",
    "\n",
    "# Plot regression line\n",
    "ax.plot(xseq, a + b * xseq, color=\"k\", lw=2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181d51f-e8a6-46d6-9127-0a08a519c935",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas was built by quant researcher Wes Mckinney in 2011.  It uses NumPy to deal with hetrogenous data (intgers, floats and strings) and makes manipulation easier, allowing excel like operations (not the UI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01999cd3-8b95-4903-8e7d-a5ec1932ab65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/paddy/code/example-notebooks/citibike-trips.csv\") \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b49e8-b5b8-4dfc-8e7b-77537fd58f18",
   "metadata": {},
   "source": [
    "# The Jupyter notebook\n",
    "\n",
    "I have been demonstrating the entire PyData ecosystem inside of the Jupyter notebook.  This is an interactive analysis and documentation environment built around python.  While a grad student Fernando Perez wanted a better [interactive environment](https://en.wikipedia.org/wiki/IPython) for playing with data in 2001.  In 2011 the IPython first released the [jupyter notebook](https://en.wikipedia.org/wiki/Project_Jupyter) interface you see here.\n",
    "\n",
    "Combining small snippets of analysis code, with charts, and narrative text allowed academics to write and share research in ways that were cumbersome before.\n",
    "(Maybe show emacs/vscode traditional method of writing code).  This is particularly important for data intensive analysis.  You need to look at the data and play with it iteratively.  This interface works very well for the problem that data scientists and academics deal with every day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cac8c9-5adc-4cdf-a409-4cb2460476a2",
   "metadata": {},
   "source": [
    "# Pandas\n",
    "\n",
    "In 2011 financial quant researcher Wes Mckinney released pandas which made analysis of realworld data easier, timeseries data in particular.  Pandas was built on top of NumPY, and allowed computations to be run on mixed datasets (you could have a set of temperature observations ordered by time of day, with a string column for location).\n",
    "\n",
    "Pandas like each of the previous tools took what was technically possible, and increased the usability so a broader audience could start doing their work in the PyData ecosystem.\n",
    "\n",
    "The following code shows reading a 300,000 csv file about citibike trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a570b8c-0a3a-4746-8de4-81dadb39c497",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/Users/paddy/code/example-notebooks/citibike-trips.csv\") \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1795cf2-bafd-4121-bb56-730037b5e646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Once you have a dataset like this there are a lot of operations you might want to perform\n",
    "df['tripduration'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c02534-b54f-41ed-b049-6b172e1274ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['start station name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9707a-e2be-4501-a9bb-25d94603d2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.groupby('start station name').mean('tripduration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35519673-7d49-42af-bc2d-de0971d38bba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tripduration'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c33b0-1f90-42f2-91a6-abb9593cf27e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tripduration'].quantile(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743a340-ee80-4fea-bdc5-2d4b51ab983d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['tripduration'].quantile(.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc050042-2c6d-4d47-adcd-139321ccb726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[(df['tripduration'] > df['tripduration'].quantile(.01)) & (df['tripduration'] < df['tripduration'].quantile(.99))]['tripduration'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269109ab-fb30-449e-9eaa-b90c8f053c25",
   "metadata": {},
   "source": [
    "# Why I wrote Buckaroo\n",
    "\n",
    "Thank you for bearing with me this far.  You now have seen the PyData ecosystem and a small sample of how it is used.\n",
    "These are all powerful tools, but a bit cumbersome to use.  I look at multiple different datasets a day, and I want to quickly understand them.  I don't want to type a bunch of commands to get the overview I'm looking for and I want to be able to look at the raw data.  Here is Buckaroo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5683c8-7cc5-4933-bd75-fd74420a81af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import buckaroo\n",
    "df = pd.read_csv(\"/Users/paddy/code/example-notebooks/citibike-trips.csv\") \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e60c0c-a37f-4365-9fac-6f5438968940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
